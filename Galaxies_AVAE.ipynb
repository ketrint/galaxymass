{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.cluster import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import astropy\n",
    "from astropy.coordinates import solar_system_ephemeris, EarthLocation\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import constants as const\n",
    "import hdbscan\n",
    "from astropy.io import fits\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "# from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /root/.astropy/cache/download/py3/0508754bf44ea7119c5b6b1a661568ef\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU      16   (73531,)   uint8   \n",
      "  1  Joined        1 BinTableHDU   1644   4105516R x 559C   [J, D, D, D, D, D, K, I, J, I, E, J, 6A, D, I, 15A, J, J, D, D, D, D, E, E, E, J, J, J, 40A, B, B, E, I, I, 10A, E, J, E, J, E, I, 14A, E, I, 33A, L, L, E, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, E, E, E, E, E, D, D, D, D, D, E, E, E, E, E, D, D, D, D, D, E, E, E, E, E, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, E, E, E, E, E, E, E, E, E, E, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, D, D, D, D, D, D, D, D, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, E, D, D, E, E, E, E, D, D, D, E, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, E, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, L, L, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D, D]   \n"
     ]
    }
   ],
   "source": [
    "# открыть файлы с скрасными смещениями\n",
    "\n",
    "data = fits.open('http://gal-03.sai.msu.ru/~vtoptun/photometry/rcsed_v2_clean.fits')\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "GrID = pd.read_csv('rcsed_iGrID.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns.names #559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# конвертация в датафрейм\n",
    "RCSED_V2 = pd.DataFrame(np.array(data).byteswap().newbyteorder())\n",
    "display(RCSED_V2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_cols = list(RCSED_V2.filter(regex='ID'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID_cols.append('ind')\n",
    "ID_cols.append('recno_uzc')\n",
    "ID_cols.append('ind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCSED_V2 = RCSED_V2[RCSED_V2.columns.drop(ID_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['SpecFile_wigglez', 'targetname_6df', 'SeqNum_2df', 'planid_lamost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCSED_V2 = RCSED_V2[RCSED_V2.columns.drop(cat_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(RCSED_V2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCSED_V2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data = len(RCSED_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(np.array(RCSED_V2.OBJNO_deep2) != -2147483648) / len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_treating(cols, null):\n",
    "    for col in cols:\n",
    "        if sum(np.array(RCSED_V2[col]) == null) / len(RCSED_V2) > 0.9:\n",
    "            RCSED_V2.drop([col],axis=1, inplace=True)\n",
    "            cols.remove(col)\n",
    "        else:\n",
    "            RCSED_V2[col].replace(null, np.nan) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null = -2147483648\n",
    "cols_NaN1 = ['mjd_sdss', 'q_z_2df', 'specid_6df', 'obsid_lamost', 'lmjd_lamost', 'mjd_lamost',\n",
    "             'OBJNO_deep2', 'OBJNO_deep3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null = -32768\n",
    "cols_NaN2 = ['plate_sdss', 'quality_6df', 'f_z_lega_c', 'Q_wigglez', 'NQ_gama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#null = 255\n",
    "cols_NaN3 = ['spid_lamost', 'fiberid_lamost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_treating(cols_NaN1, -2147483648)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_treating(cols_NaN2, -32768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_treating(cols_NaN3, 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(RCSED_V2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Anomaly detection\n",
    "\n",
    "top_values = []\n",
    "nan = []\n",
    "\n",
    "for col in RCSED_V2.columns:\n",
    "    vc = RCSED_V2[col].value_counts(ascending=False).head(1)\n",
    "    nan = (RCSED_V2[col].isna()).sum()\n",
    "    total = nan\n",
    "    if vc[vc.index[0]] >1000:\n",
    "        total = vc[vc.index[0]]+nan\n",
    "    top_values.append([vc.name, vc.index[0], vc[vc.index[0]], nan, total])\n",
    "    \n",
    "anomaly = pd.DataFrame(data=top_values, columns = ['column_name', 'anomaly_value', 'value_counts', 'nan', 'total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly.sort_values(by='total', ascending=False)[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4105516*0.9 = 3694964\n",
    "\n",
    "col_anomaly = anomaly[anomaly.total>3694964].column_name.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCSED_V2.drop(col_anomaly, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(RCSED_V2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(RCSED_V2.w3sigmag_7_wise)\n",
    "plt.title('w3sigmag_7_wise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(RCSED_V2.w3sigmag_7_wise))-2202889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do**\n",
    "*IterativeImputer*\n",
    "-> models each feature with missing values as a function of other features, and uses that estimate for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(RCSED_V2.KAPERMAG3ERR_ukidss))-2967341"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_for_nan = anomaly2.sort_values(by='total', ascending=False)[1:149].column_name.to_list()\n",
    "col_anomaly = anomaly2.sort_values(by='total', ascending=False)[1:149].anomaly_value.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, anomaly in zip(col_for_nan, col_anomaly):\n",
    "    RCSED_V2[col].replace(anomaly, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = RCSED_V2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0 = data_0.replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeled part of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GrID.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GrID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdss_indx = GrID[-GrID.iGrID.isna()].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sdss_indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0.ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_0[data_0.ind.isin(sdss_indx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = GrID.iloc[sdss_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(y.iGrID).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sized = y[y.iGrID.isin(s[s == 1].index)].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_without_one = y[~y.iGrID.isin(s[s == 1].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_without_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_without_one = x[~x.ind.isin(one_sized)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_without_one.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.96068453e+00, -8.47137893e-01,  1.36122092e+00, ...,\n",
       "         5.88998002e-05,  3.56226173e-01,  2.54646558e-04],\n",
       "       [-2.96064404e+00, -1.35645424e-01,  1.95708374e+00, ...,\n",
       "         3.61292801e-03,  4.12418385e-01,  2.47764867e-03],\n",
       "       [-2.96063036e+00, -1.40620777e+00,  2.54383877e+00, ...,\n",
       "         8.25179404e-04,  3.93781511e-01,  9.52398655e-04],\n",
       "       ...,\n",
       "       [ 2.80389843e+00, -1.01314244e-01, -5.85270884e-02, ...,\n",
       "        -9.70410412e-04, -4.84952697e-01, -6.00629403e-04],\n",
       "       [ 2.80390023e+00, -8.60379393e-01,  3.69015423e-02, ...,\n",
       "        -9.70410412e-04, -4.84952697e-01, -6.00629403e-04],\n",
       "       [ 2.80390203e+00, -4.82020008e-01, -4.40446706e-01, ...,\n",
       "        -3.39085502e-04,  2.85324190e-01,  2.75540075e-05]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x_without_one)\n",
    "display(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220777, 347)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array(y_without_one).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220777,)"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return torch.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, x.shape[1]))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(x_dim=x.shape[1], h_dim1= int(x.shape[1]/2), h_dim2=int(x.shape[1]/4), z_dim=1)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=x, batch_size=int(len(x)/1000), shuffle=True)#, num_workers=20)\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    criterion = nn.MSELoss().cuda()\n",
    "    BCE = criterion(recon_x, x.view(-1, x.shape[1]))\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx in range(400):\n",
    "        #data = torch.tensor(x.sample(n=1000).astype(np.float32).values)\n",
    "        data = torch.tensor(x[np.random.choice(range(x.shape[0]), 1000)].astype(np.float32))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d76422bdcd45859875f0bfd996f067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=24.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/220777 (0%)]\tLoss: 0.002165\n",
      "Train Epoch: 1 [100000/220777 (10%)]\tLoss: 0.001440\n",
      "Train Epoch: 1 [200000/220777 (20%)]\tLoss: 0.002923\n",
      "Train Epoch: 1 [300000/220777 (30%)]\tLoss: 0.001182\n",
      "====> Epoch: 1 Average loss: 89975084.6788\n",
      "Train Epoch: 2 [0/220777 (0%)]\tLoss: 0.001076\n",
      "Train Epoch: 2 [100000/220777 (10%)]\tLoss: 0.013517\n",
      "Train Epoch: 2 [200000/220777 (20%)]\tLoss: 0.004049\n",
      "Train Epoch: 2 [300000/220777 (30%)]\tLoss: 0.001038\n",
      "====> Epoch: 2 Average loss: 0.0885\n",
      "Train Epoch: 3 [0/220777 (0%)]\tLoss: 0.001133\n",
      "Train Epoch: 3 [100000/220777 (10%)]\tLoss: 0.000955\n",
      "Train Epoch: 3 [200000/220777 (20%)]\tLoss: 0.000953\n",
      "Train Epoch: 3 [300000/220777 (30%)]\tLoss: 0.001516\n",
      "====> Epoch: 3 Average loss: 0.0384\n",
      "Train Epoch: 4 [0/220777 (0%)]\tLoss: 0.001392\n",
      "Train Epoch: 4 [100000/220777 (10%)]\tLoss: 0.000907\n",
      "Train Epoch: 4 [200000/220777 (20%)]\tLoss: 0.000791\n",
      "Train Epoch: 4 [300000/220777 (30%)]\tLoss: 0.000656\n",
      "====> Epoch: 4 Average loss: 0.0041\n",
      "Train Epoch: 5 [0/220777 (0%)]\tLoss: 0.000743\n",
      "Train Epoch: 5 [100000/220777 (10%)]\tLoss: 0.002155\n",
      "Train Epoch: 5 [200000/220777 (20%)]\tLoss: 0.000873\n",
      "Train Epoch: 5 [300000/220777 (30%)]\tLoss: 0.001180\n",
      "====> Epoch: 5 Average loss: 1.1547\n",
      "Train Epoch: 6 [0/220777 (0%)]\tLoss: 0.001498\n",
      "Train Epoch: 6 [100000/220777 (10%)]\tLoss: 0.001062\n",
      "Train Epoch: 6 [200000/220777 (20%)]\tLoss: 0.000903\n",
      "Train Epoch: 6 [300000/220777 (30%)]\tLoss: 0.000745\n",
      "====> Epoch: 6 Average loss: 0.0483\n",
      "Train Epoch: 7 [0/220777 (0%)]\tLoss: 0.001951\n",
      "Train Epoch: 7 [100000/220777 (10%)]\tLoss: 0.001057\n",
      "Train Epoch: 7 [200000/220777 (20%)]\tLoss: 0.001216\n",
      "Train Epoch: 7 [300000/220777 (30%)]\tLoss: 0.001369\n",
      "====> Epoch: 7 Average loss: 0.0798\n",
      "Train Epoch: 8 [0/220777 (0%)]\tLoss: 0.001349\n",
      "Train Epoch: 8 [100000/220777 (10%)]\tLoss: 0.000910\n",
      "Train Epoch: 8 [200000/220777 (20%)]\tLoss: 0.002642\n",
      "Train Epoch: 8 [300000/220777 (30%)]\tLoss: 0.002136\n",
      "====> Epoch: 8 Average loss: 0.2731\n",
      "Train Epoch: 9 [0/220777 (0%)]\tLoss: 0.002295\n",
      "Train Epoch: 9 [100000/220777 (10%)]\tLoss: 0.001407\n",
      "Train Epoch: 9 [200000/220777 (20%)]\tLoss: 0.000888\n",
      "Train Epoch: 9 [300000/220777 (30%)]\tLoss: 0.000795\n",
      "====> Epoch: 9 Average loss: 0.0114\n",
      "Train Epoch: 10 [0/220777 (0%)]\tLoss: 0.000822\n",
      "Train Epoch: 10 [100000/220777 (10%)]\tLoss: 0.000919\n",
      "Train Epoch: 10 [200000/220777 (20%)]\tLoss: 0.001109\n",
      "Train Epoch: 10 [300000/220777 (30%)]\tLoss: 0.001026\n",
      "====> Epoch: 10 Average loss: 0.0045\n",
      "Train Epoch: 11 [0/220777 (0%)]\tLoss: 0.000794\n",
      "Train Epoch: 11 [100000/220777 (10%)]\tLoss: 0.000638\n",
      "Train Epoch: 11 [200000/220777 (20%)]\tLoss: 0.005930\n",
      "Train Epoch: 11 [300000/220777 (30%)]\tLoss: 0.001066\n",
      "====> Epoch: 11 Average loss: 0.0030\n",
      "Train Epoch: 12 [0/220777 (0%)]\tLoss: 0.000665\n",
      "Train Epoch: 12 [100000/220777 (10%)]\tLoss: 0.000728\n",
      "Train Epoch: 12 [200000/220777 (20%)]\tLoss: 0.006508\n",
      "Train Epoch: 12 [300000/220777 (30%)]\tLoss: 0.001231\n",
      "====> Epoch: 12 Average loss: 0.0087\n",
      "Train Epoch: 13 [0/220777 (0%)]\tLoss: 0.002025\n",
      "Train Epoch: 13 [100000/220777 (10%)]\tLoss: 0.001223\n",
      "Train Epoch: 13 [200000/220777 (20%)]\tLoss: 0.000658\n",
      "Train Epoch: 13 [300000/220777 (30%)]\tLoss: 0.000670\n",
      "====> Epoch: 13 Average loss: 0.0084\n",
      "Train Epoch: 14 [0/220777 (0%)]\tLoss: 0.013397\n",
      "Train Epoch: 14 [100000/220777 (10%)]\tLoss: 0.001051\n",
      "Train Epoch: 14 [200000/220777 (20%)]\tLoss: 0.000726\n",
      "Train Epoch: 14 [300000/220777 (30%)]\tLoss: 0.000689\n",
      "====> Epoch: 14 Average loss: 0.0232\n",
      "Train Epoch: 15 [0/220777 (0%)]\tLoss: 0.001306\n",
      "Train Epoch: 15 [100000/220777 (10%)]\tLoss: 0.001531\n",
      "Train Epoch: 15 [200000/220777 (20%)]\tLoss: 0.001490\n",
      "Train Epoch: 15 [300000/220777 (30%)]\tLoss: 0.000927\n",
      "====> Epoch: 15 Average loss: 0.0968\n",
      "Train Epoch: 16 [0/220777 (0%)]\tLoss: 0.000873\n",
      "Train Epoch: 16 [100000/220777 (10%)]\tLoss: 0.002669\n",
      "Train Epoch: 16 [200000/220777 (20%)]\tLoss: 0.000959\n",
      "Train Epoch: 16 [300000/220777 (30%)]\tLoss: 0.000728\n",
      "====> Epoch: 16 Average loss: 0.0460\n",
      "Train Epoch: 17 [0/220777 (0%)]\tLoss: 0.000917\n",
      "Train Epoch: 17 [100000/220777 (10%)]\tLoss: 0.000854\n",
      "Train Epoch: 17 [200000/220777 (20%)]\tLoss: 0.001095\n",
      "Train Epoch: 17 [300000/220777 (30%)]\tLoss: 0.000755\n",
      "====> Epoch: 17 Average loss: 0.0635\n",
      "Train Epoch: 18 [0/220777 (0%)]\tLoss: 0.000677\n",
      "Train Epoch: 18 [100000/220777 (10%)]\tLoss: 0.000698\n",
      "Train Epoch: 18 [200000/220777 (20%)]\tLoss: 0.000645\n",
      "Train Epoch: 18 [300000/220777 (30%)]\tLoss: 0.000658\n",
      "====> Epoch: 18 Average loss: 0.0051\n",
      "Train Epoch: 19 [0/220777 (0%)]\tLoss: 0.000675\n",
      "Train Epoch: 19 [100000/220777 (10%)]\tLoss: 0.000940\n",
      "Train Epoch: 19 [200000/220777 (20%)]\tLoss: 0.000675\n",
      "Train Epoch: 19 [300000/220777 (30%)]\tLoss: 0.000914\n",
      "====> Epoch: 19 Average loss: 0.0041\n",
      "Train Epoch: 20 [0/220777 (0%)]\tLoss: 0.000713\n",
      "Train Epoch: 20 [100000/220777 (10%)]\tLoss: 0.000759\n",
      "Train Epoch: 20 [200000/220777 (20%)]\tLoss: 0.025784\n",
      "Train Epoch: 20 [300000/220777 (30%)]\tLoss: 0.000725\n",
      "====> Epoch: 20 Average loss: 0.0034\n",
      "Train Epoch: 21 [0/220777 (0%)]\tLoss: 0.000617\n",
      "Train Epoch: 21 [100000/220777 (10%)]\tLoss: 0.000801\n",
      "Train Epoch: 21 [200000/220777 (20%)]\tLoss: 0.000621\n",
      "Train Epoch: 21 [300000/220777 (30%)]\tLoss: 0.000838\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-459-9d9ae06cbe9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-457-409f054b47eb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#data = torch.tensor(x.sample(n=1000).astype(np.float32).values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in tqdm(range(1, 25)):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, log_var = vae.encoder(torch.tensor(x.astype(np.float32)))\n",
    "embed = vae.sampling(mu, log_var).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbScan = hdbscan.hdbscan_.HDBSCAN(min_cluster_size=2, min_samples=10, \n",
    "                                   allow_single_cluster=False, core_dist_n_jobs=20).fit(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5330250424733153, 0.7729184646698886, 0.6309383142618596)"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = homogeneity_completeness_v_measure(Y,hdbScan.labels_)\n",
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1005      150\n",
       " 1234      155\n",
       " 3281      158\n",
       " 1302      166\n",
       " 230       168\n",
       " 1595      179\n",
       " 2321      193\n",
       " 1780      199\n",
       " 1788      203\n",
       " 5106      209\n",
       " 1348      210\n",
       " 70        231\n",
       " 1211      244\n",
       " 912       485\n",
       "-1       52273\n",
       "dtype: int64"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(hdbScan.labels_).value_counts(ascending=True).tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191838.0    166\n",
       "101773.0    176\n",
       "176164.0    178\n",
       "20078.0     181\n",
       "20266.0     208\n",
       "13370.0     219\n",
       "160410.0    221\n",
       "127065.0    223\n",
       "30885.0     230\n",
       "112271.0    239\n",
       "42643.0     266\n",
       "176804.0    389\n",
       "39456.0     547\n",
       "153084.0    739\n",
       "149473.0    861\n",
       "dtype: int64"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(Y).value_counts(ascending=True).tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 1\n",
    "input_size = x.shape[1]\n",
    "n_samples = x.shape[0]\n",
    "\n",
    "h_dim1 = int(x.shape[1]/2)\n",
    "h_dim2 = int(x.shape[1]/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220777"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AVAE, self).__init__()\n",
    "        \n",
    "        self.gen_l1 = torch.nn.Linear(z_dim, h_dim2)\n",
    "        self.gen_l2 = torch.nn.Linear(h_dim2, h_dim1)\n",
    "        self.gen_l3 = torch.nn.Linear(h_dim1, input_size)\n",
    "        \n",
    "        self.enc_l1 = torch.nn.Linear(input_size+z_dim, h_dim1)\n",
    "        self.enc_l2 = torch.nn.Linear(h_dim1, h_dim2)\n",
    "        self.enc_l3 = torch.nn.Linear(h_dim2, z_dim)\n",
    "        \n",
    "        self.disc_l1 = torch.nn.Linear(input_size+z_dim, h_dim1)\n",
    "        self.disc_l2 = torch.nn.Linear(h_dim1, h_dim2)\n",
    "        self.disc_l3 = torch.nn.Linear(h_dim2, 1)\n",
    "    \n",
    "    def sample_prior(self, s):\n",
    "        if self.training:\n",
    "            m = torch.zeros((s.data.shape[0], z_dim))\n",
    "            std = torch.ones((s.data.shape[0], z_dim))\n",
    "            d = Variable(torch.normal(m,std))\n",
    "        else:\n",
    "            d = Variable(torch.zeros((s.data.shape[0], z_dim)))\n",
    "            \n",
    "        return d\n",
    "        \n",
    "    def discriminator(self, x,z):\n",
    "        i = torch.cat((x, z), dim=1)\n",
    "        h = F.relu(self.disc_l1(i))\n",
    "        h = F.relu(self.disc_l2(h))\n",
    "        return self.disc_l3(h)  \n",
    "        \n",
    "    def sample_posterior(self, x):\n",
    "        i = torch.cat((x, self.sample_prior(x)), dim=1)\n",
    "        h = F.relu(self.enc_l1(i))\n",
    "        h = F.relu(self.enc_l2(h))\n",
    "        return self.enc_l3(h)\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        i = F.relu(self.gen_l1(z))\n",
    "        h = F.relu(self.gen_l2(i))\n",
    "        return torch.sigmoid(self.gen_l3(h))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, x.shape[1]))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_p = self.sample_prior(x)\n",
    "        \n",
    "        z_q = self.sample_posterior(x)\n",
    "        \n",
    "        log_d_prior = self.discriminator(x, z_p)\n",
    "        log_d_posterior = self.discriminator(x, z_q)\n",
    "        \n",
    "        disc_loss = torch.mean(\n",
    "            nn.functional.binary_cross_entropy_with_logits(\n",
    "            log_d_posterior, torch.ones_like(log_d_posterior)\n",
    "        )\n",
    "        + nn.functional.binary_cross_entropy_with_logits(\n",
    "            log_d_prior, torch.zeros_like(log_d_prior))\n",
    "        )\n",
    "        \n",
    "        x_recon = self.decoder(z_q)\n",
    "        recon_liklihood = -nn.functional.binary_cross_entropy(\n",
    "                                                x_recon, x)*x.data.shape[0]\n",
    "        \n",
    "        gen_loss = torch.mean(log_d_posterior)-torch.mean(recon_liklihood)\n",
    "        \n",
    "        return disc_loss, gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_params = []\n",
    "gen_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    \n",
    "    if 'disc' in name:\n",
    "        \n",
    "        disc_params.append(param)\n",
    "    else:\n",
    "        gen_params.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_optimizer = torch.optim.Adam(disc_params, lr=1e-3)\n",
    "gen_optimizer = torch.optim.Adam(gen_params, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220777, 347)"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AVAE(epoch, batches_per_epoch = 501, log_interval=500):\n",
    "    model.train()\n",
    "    \n",
    "    ind = np.arange(x.shape[0])\n",
    "    for i in range(batches_per_epoch):\n",
    "        data = torch.from_numpy(x[np.random.choice(ind, size=batch_size)])\n",
    "        data = Variable(data, requires_grad=False).float()\n",
    "        \n",
    "        \n",
    "        discrim_loss, gen_loss= model(data)\n",
    "        \n",
    "        gen_optimizer.zero_grad()\n",
    "        gen_loss.backward(retain_graph=True)\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        disc_optimizer.zero_grad()\n",
    "        discrim_loss.backward(retain_graph=True)\n",
    "        disc_optimizer.step()\n",
    "        if (i % log_interval == 0) and (epoch % 1 ==0):\n",
    "            #Print progress\n",
    "            print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}\\tLoss: {:.6f}'.format(\n",
    "                epoch, i * batch_size, batch_size*batches_per_epoch,\n",
    "                discrim_loss.item() / len(data), gen_loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} done!'.format(\n",
    "          epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/128256]\tLoss: 0.001176\tLoss: -4.678825\n",
      "Train Epoch: 1 [128000/128256]\tLoss: 0.001314\tLoss: -4.578405\n",
      "====> Epoch: 1 done!\n",
      "Train Epoch: 2 [0/128256]\tLoss: 0.001075\tLoss: -4.614848\n",
      "Train Epoch: 2 [128000/128256]\tLoss: 0.000906\tLoss: -4.888037\n",
      "====> Epoch: 2 done!\n",
      "Train Epoch: 3 [0/128256]\tLoss: 0.001257\tLoss: -5.129023\n",
      "Train Epoch: 3 [128000/128256]\tLoss: 0.001005\tLoss: -4.264121\n",
      "====> Epoch: 3 done!\n",
      "Train Epoch: 4 [0/128256]\tLoss: 0.001019\tLoss: -4.223755\n",
      "Train Epoch: 4 [128000/128256]\tLoss: 0.000969\tLoss: -4.254047\n",
      "====> Epoch: 4 done!\n",
      "Train Epoch: 5 [0/128256]\tLoss: 0.001105\tLoss: -4.278278\n",
      "Train Epoch: 5 [128000/128256]\tLoss: 0.001202\tLoss: -4.698373\n",
      "====> Epoch: 5 done!\n",
      "Train Epoch: 6 [0/128256]\tLoss: 0.001199\tLoss: -5.099048\n",
      "Train Epoch: 6 [128000/128256]\tLoss: 0.000863\tLoss: -4.109143\n",
      "====> Epoch: 6 done!\n",
      "Train Epoch: 7 [0/128256]\tLoss: 0.001410\tLoss: -4.537691\n",
      "Train Epoch: 7 [128000/128256]\tLoss: 0.000979\tLoss: -5.109467\n",
      "====> Epoch: 7 done!\n",
      "Train Epoch: 8 [0/128256]\tLoss: 0.000968\tLoss: -4.538981\n",
      "Train Epoch: 8 [128000/128256]\tLoss: 0.001066\tLoss: -4.449981\n",
      "====> Epoch: 8 done!\n",
      "Train Epoch: 9 [0/128256]\tLoss: 0.001064\tLoss: -4.543783\n",
      "Train Epoch: 9 [128000/128256]\tLoss: 0.000820\tLoss: -4.963132\n",
      "====> Epoch: 9 done!\n",
      "Train Epoch: 10 [0/128256]\tLoss: 0.001305\tLoss: -4.663982\n",
      "Train Epoch: 10 [128000/128256]\tLoss: 0.001481\tLoss: -5.352352\n",
      "====> Epoch: 10 done!\n",
      "Train Epoch: 11 [0/128256]\tLoss: 0.000917\tLoss: -4.476896\n",
      "Train Epoch: 11 [128000/128256]\tLoss: 0.000959\tLoss: -4.691401\n",
      "====> Epoch: 11 done!\n",
      "Train Epoch: 12 [0/128256]\tLoss: 0.001196\tLoss: -4.867191\n",
      "Train Epoch: 12 [128000/128256]\tLoss: 0.000613\tLoss: -4.802677\n",
      "====> Epoch: 12 done!\n",
      "Train Epoch: 13 [0/128256]\tLoss: 0.000788\tLoss: -5.193784\n",
      "Train Epoch: 13 [128000/128256]\tLoss: 0.001483\tLoss: -4.504705\n",
      "====> Epoch: 13 done!\n",
      "Train Epoch: 14 [0/128256]\tLoss: 0.001214\tLoss: -4.836433\n",
      "Train Epoch: 14 [128000/128256]\tLoss: 0.000823\tLoss: -4.805537\n",
      "====> Epoch: 14 done!\n",
      "CPU times: user 34min 39s, sys: 7min 29s, total: 42min 8s\n",
      "Wall time: 8min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, 15):\n",
    "    train_AVAE(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Variable(torch.from_numpy(x), requires_grad=False)\n",
    "\n",
    "model.train()\n",
    "zs = model.sample_posterior(data.float()).detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "#plt.scatter(zs[:,0], zs[:, 1], c=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220777, 1)"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220777, 347)"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "1) grid search for min_samples & min_cluster_size\n",
    "\n",
    "2) optimize AVAE + graphics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdbScan2 = hdbscan.hdbscan_.HDBSCAN(min_cluster_size=2, min_samples=9, \n",
    "                                   allow_single_cluster=False, core_dist_n_jobs=20).fit(zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220777"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hdbScan2.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8438774646309881, 0.8285269211292502, 0.8361317436550977)"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4 = homogeneity_completeness_v_measure(Y,hdbScan2.labels_) \n",
    "m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72113"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(hdbScan2.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56546"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(Y)) #true number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 2737      190\n",
       " 7213      202\n",
       " 8573      207\n",
       " 6756      214\n",
       " 5223      217\n",
       " 5605      218\n",
       " 3819      229\n",
       " 5064      237\n",
       " 8695      260\n",
       " 8878      273\n",
       " 5456      299\n",
       " 505       338\n",
       " 501       759\n",
       " 3142      826\n",
       "-1       51137\n",
       "dtype: int64"
      ]
     },
     "execution_count": 574,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#9\n",
    "pd.Series(hdbScan2.labels_).value_counts(ascending=True).tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = pd.DataFrame(hdbScan2.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = ls[ls[0] == -1].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191838.0    166\n",
       "101773.0    176\n",
       "176164.0    178\n",
       "20078.0     181\n",
       "20266.0     208\n",
       "13370.0     219\n",
       "160410.0    221\n",
       "127065.0    223\n",
       "30885.0     230\n",
       "112271.0    239\n",
       "42643.0     266\n",
       "176804.0    389\n",
       "39456.0     547\n",
       "153084.0    739\n",
       "149473.0    861\n",
       "dtype: int64"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(Y).value_counts(ascending=True).tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
